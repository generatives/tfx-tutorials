{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.30.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Definition of Beam TFX runner.\"\"\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "from absl import logging\n",
    "from tfx.dsl.compiler import compiler\n",
    "from tfx.dsl.compiler import constants\n",
    "from tfx.dsl.components.base import base_component\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline as pipeline_py\n",
    "from tfx.orchestration.local import runner_utils\n",
    "from tfx.orchestration.portable import launcher\n",
    "from tfx.orchestration.portable import runtime_parameter_utils\n",
    "from tfx.orchestration.portable import tfx_runner\n",
    "from tfx.utils import telemetry_utils\n",
    "\n",
    "\n",
    "class CustomLocalDagRunner(tfx_runner.TfxRunner):\n",
    "  \"\"\"Local TFX DAG runner using a runtime id compatible with Windows folder naming.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def run(self, pipeline: pipeline_py.Pipeline) -> None:\n",
    "    \"\"\"Runs given logical pipeline locally.\n",
    "    Args:\n",
    "      pipeline: Logical pipeline containing pipeline args and components.\n",
    "    \"\"\"\n",
    "    for component in pipeline.components:\n",
    "      # TODO(b/187122662): Pass through pip dependencies as a first-class\n",
    "      # component flag.\n",
    "      if isinstance(component, base_component.BaseComponent):\n",
    "        component._resolve_pip_dependencies(  # pylint: disable=protected-access\n",
    "            pipeline.pipeline_info.pipeline_root)\n",
    "\n",
    "    c = compiler.Compiler()\n",
    "    pipeline = c.compile(pipeline)\n",
    "\n",
    "    # Substitute the runtime parameter to be a concrete run_id\n",
    "    runtime_parameter_utils.substitute_runtime_parameter(\n",
    "        pipeline, {\n",
    "            constants.PIPELINE_RUN_ID_PARAMETER_NAME:\n",
    "                datetime.datetime.now().strftime('%d-%m-%YT%H.%M.%S.%f'),\n",
    "        })\n",
    "\n",
    "    deployment_config = runner_utils.extract_local_deployment_config(pipeline)\n",
    "    connection_config = deployment_config.metadata_connection_config\n",
    "\n",
    "    logging.info('Running pipeline:\\n %s', pipeline)\n",
    "    logging.info('Using deployment config:\\n %s', deployment_config)\n",
    "    logging.info('Using connection config:\\n %s', connection_config)\n",
    "\n",
    "    with telemetry_utils.scoped_labels(\n",
    "        {telemetry_utils.LABEL_TFX_RUNNER: 'local'}):\n",
    "      # Run each component. Note that the pipeline.components list is in\n",
    "      # topological order.\n",
    "      #\n",
    "      # TODO(b/171319478): After IR-based execution is used, used multi-threaded\n",
    "      # execution so that independent components can be run in parallel.\n",
    "      for node in pipeline.nodes:\n",
    "        pipeline_node = node.pipeline_node\n",
    "        node_id = pipeline_node.node_info.id\n",
    "        executor_spec = runner_utils.extract_executor_spec(\n",
    "            deployment_config, node_id)\n",
    "        custom_driver_spec = runner_utils.extract_custom_driver_spec(\n",
    "            deployment_config, node_id)\n",
    "\n",
    "        component_launcher = launcher.Launcher(\n",
    "            pipeline_node=pipeline_node,\n",
    "            mlmd_connection=metadata.Metadata(connection_config),\n",
    "            pipeline_info=pipeline.pipeline_info,\n",
    "            pipeline_runtime_spec=pipeline.runtime_spec,\n",
    "            executor_spec=executor_spec,\n",
    "            custom_driver_spec=custom_driver_spec)\n",
    "        logging.info('Component %s is running.', node_id)\n",
    "        component_launcher.launch()\n",
    "        logging.info('Component %s is finished.', node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"penguin-tfma\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('./pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('./metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('./serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tfx-datar5h1vbce\\\\data.csv',\n",
       " <http.client.HTTPMessage at 0x13c1c043430>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
    "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
    "urllib.request.urlretrieve(_data_url, _data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'penguin_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguin_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      dataset_options.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "  # NEW: Get the latest blessed model for Evaluator.\n",
    "  model_resolver = tfx.dsl.Resolver(\n",
    "      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "      model_blessing=tfx.dsl.Channel(\n",
    "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "              'latest_blessed_model_resolver')\n",
    "\n",
    "  # NEW: Uses TFMA to compute evaluation statistics over features of a model and\n",
    "  #   perform quality validation of a candidate model (compared to a baseline).\n",
    "\n",
    "  eval_config = tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key='species')],\n",
    "      slicing_specs=[\n",
    "          # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
    "          tfma.SlicingSpec(),\n",
    "          # Calculate metrics for each penguin species.\n",
    "          tfma.SlicingSpec(feature_keys=['species']),\n",
    "          ],\n",
    "      metrics_specs=[\n",
    "          tfma.MetricsSpec(per_slice_thresholds={\n",
    "              'sparse_categorical_accuracy':\n",
    "                  tfma.config.PerSliceMetricThresholds(thresholds=[\n",
    "                      tfma.PerSliceMetricThreshold(\n",
    "                          slicing_specs=[tfma.SlicingSpec()],\n",
    "                          threshold=tfma.MetricThreshold(\n",
    "                              value_threshold=tfma.GenericValueThreshold(\n",
    "                                   lower_bound={'value': 0.6}),\n",
    "                              # Change threshold will be ignored if there is no\n",
    "                              # baseline model resolved from MLMD (first run).\n",
    "                              change_threshold=tfma.GenericChangeThreshold(\n",
    "                                  direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                                  absolute={'value': -1e-10}))\n",
    "                       )]),\n",
    "          })],\n",
    "      )\n",
    "  evaluator = tfx.components.Evaluator(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      model=trainer.outputs['model'],\n",
    "      baseline_model=model_resolver.outputs['model'],\n",
    "      eval_config=eval_config)\n",
    "\n",
    "  # Checks whether the model passed the validation steps and pushes the model\n",
    "  # to a file destination if check passed.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      model_blessing=evaluator.outputs['blessing'], # Pass an evaluation result.\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "\n",
    "      # Following two components were added to the pipeline.\n",
    "      model_resolver,\n",
    "      evaluator,\n",
    "\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating ephemeral wheel package for 'C:\\\\Users\\\\deaston\\\\Documents\\\\Python Scripts\\\\TFXPipeline\\\\Tutorial_4\\\\penguin_trainer.py' (including modules: ['penguin_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version 07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699.\n",
      "INFO:absl:Executing: ['C:\\\\Users\\\\deaston\\\\Anaconda3\\\\python.exe', 'C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tmpfsvo4c7u\\\\_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', 'C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tmpjw3mx69_', '--dist-dir', 'C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tmp1niheu4w']\n",
      "INFO:absl:Successfully built user code wheel distribution at './pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl'; target user module is 'penguin_trainer'.\n",
      "INFO:absl:Full user module path is 'penguin_trainer@./pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl'\n",
      "INFO:absl:Running pipeline:\n",
      " pipeline_info {\n",
      "  id: \"penguin-tfma\"\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "      }\n",
      "      id: \"CsvExampleGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"18-06-2021T14.04.53.354211\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_base\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tfx-datar5h1vbce\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "      }\n",
      "      id: \"latest_blessed_model_resolver\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"18-06-2021T14.04.53.354211\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      resolver_config {\n",
      "        resolver_steps {\n",
      "          class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "          config_json: \"{}\"\n",
      "          input_keys: \"model\"\n",
      "          input_keys: \"model_blessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.trainer.component.Trainer\"\n",
      "      }\n",
      "      id: \"Trainer\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"18-06-2021T14.04.53.354211\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma.Trainer\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"CsvExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"18-06-2021T14.04.53.354211\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Model\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"model_run\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelRun\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"eval_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"module_path\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"penguin_trainer@./pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"train_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"CsvExampleGen\"\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    downstream_nodes: \"Pusher\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "      }\n",
      "      id: \"Evaluator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"18-06-2021T14.04.53.354211\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma.Evaluator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"baseline_model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"latest_blessed_model_resolver\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"18-06-2021T14.04.53.354211\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"CsvExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"18-06-2021T14.04.53.354211\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Trainer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"18-06-2021T14.04.53.354211\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma.Trainer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"blessing\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelBlessing\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"evaluation\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelEvaluation\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"eval_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"example_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"CsvExampleGen\"\n",
      "    upstream_nodes: \"Trainer\"\n",
      "    upstream_nodes: \"latest_blessed_model_resolver\"\n",
      "    downstream_nodes: \"Pusher\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.pusher.component.Pusher\"\n",
      "      }\n",
      "      id: \"Pusher\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"18-06-2021T14.04.53.354211\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"penguin-tfma.Pusher\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Trainer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"18-06-2021T14.04.53.354211\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma.Trainer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Evaluator\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"18-06-2021T14.04.53.354211\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"penguin-tfma.Evaluator\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"blessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"pushed_model\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"PushedModel\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"push_destination\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\\\\\\penguin-tfma\\\"\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"Evaluator\"\n",
      "    upstream_nodes: \"Trainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "runtime_spec {\n",
      "  pipeline_root {\n",
      "    field_value {\n",
      "      string_value: \"./pipelines\\\\penguin-tfma\"\n",
      "    }\n",
      "  }\n",
      "  pipeline_run_id {\n",
      "    field_value {\n",
      "      string_value: \"18-06-2021T14.04.53.354211\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_mode: SYNC\n",
      "deployment_config {\n",
      "  [type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig] {\n",
      "    executor_specs {\n",
      "      key: \"CsvExampleGen\"\n",
      "      value {\n",
      "        [type.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec] {\n",
      "          python_executor_spec {\n",
      "            class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    executor_specs {\n",
      "      key: \"Evaluator\"\n",
      "      value {\n",
      "        [type.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec] {\n",
      "          python_executor_spec {\n",
      "            class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    executor_specs {\n",
      "      key: \"Pusher\"\n",
      "      value {\n",
      "        [type.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec] {\n",
      "          class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    executor_specs {\n",
      "      key: \"Trainer\"\n",
      "      value {\n",
      "        [type.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec] {\n",
      "          class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    custom_driver_specs {\n",
      "      key: \"CsvExampleGen\"\n",
      "      value {\n",
      "        [type.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec] {\n",
      "          class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    metadata_connection_config {\n",
      "      [type.googleapis.com/ml_metadata.ConnectionConfig] {\n",
      "        sqlite {\n",
      "          filename_uri: \"./metadata\\\\penguin-tfma\\\\metadata.db\"\n",
      "          connection_mode: READWRITE_OPENCREATE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Evaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  sqlite {\n",
      "    filename_uri: \"./metadata\\\\penguin-tfma\\\\metadata.db\"\n",
      "    connection_mode: READWRITE_OPENCREATE\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"./metadata\\\\penguin-tfma\\\\metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tfx-datar5h1vbce\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 21\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=21, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\examples\\\\21\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1624048269,sum_checksum:1624048269\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}), exec_properties={'input_base': 'C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tfx-datar5h1vbce', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1624048269,sum_checksum:1624048269'}, execution_output_uri='./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\.system\\\\executor_execution\\\\21\\\\executor_output.pb', stateful_working_dir='./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\.system\\\\stateful_working_dir\\\\18-06-2021T14.04.53.354211', tmp_dir='./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\.system\\\\executor_execution\\\\21\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tfx-datar5h1vbce\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfma\"\n",
      ", pipeline_run_id='18-06-2021T14.04.53.354211')\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data C:\\Users\\deaston\\AppData\\Local\\Temp\\tfx-datar5h1vbce\\* to TFExample.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 21 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\examples\\\\21\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1624048269,sum_checksum:1624048269\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}) for execution 21\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component latest_blessed_model_resolver is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"latest_blessed_model_resolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an resolver node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component latest_blessed_model_resolver is finished.\n",
      "INFO:absl:Component Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"penguin_trainer@./pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 23\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=23, input_dict={'examples': [Artifact(artifact: id: 25\n",
      "type_id: 6\n",
      "uri: \"./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\examples\\\\21\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1624048269,sum_checksum:1624048269\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1624050294923\n",
      "last_update_time_since_epoch: 1624050294923\n",
      ", artifact_type: id: 6\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\23\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Model\"\n",
      ")], 'model_run': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model_run\\\\23\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}), exec_properties={'train_args': '{\\n  \"num_steps\": 100\\n}', 'eval_args': '{\\n  \"num_steps\": 5\\n}', 'custom_config': 'null', 'module_path': 'penguin_trainer@./pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl'}, execution_output_uri='./pipelines\\\\penguin-tfma\\\\Trainer\\\\.system\\\\executor_execution\\\\23\\\\executor_output.pb', stateful_working_dir='./pipelines\\\\penguin-tfma\\\\Trainer\\\\.system\\\\stateful_working_dir\\\\18-06-2021T14.04.53.354211', tmp_dir='./pipelines\\\\penguin-tfma\\\\Trainer\\\\.system\\\\executor_execution\\\\23\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"penguin_trainer@./pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfma\"\n",
      ", pipeline_run_id='18-06-2021T14.04.53.354211')\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\deaston\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-1e777c5a-0bd6-4d17-b1b3-980ff015004f.json']\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir C:\\Users\\deaston\\Anaconda3\\lib\\site-packages\\tfx to temp dir C:\\Users\\deaston\\AppData\\Local\\Temp\\tmp1vjjc3n7\\build\\tfx\n",
      "INFO:absl:Generating a temp setup file at C:\\Users\\deaston\\AppData\\Local\\Temp\\tmp1vjjc3n7\\build\\tfx\\setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at C:\\Users\\deaston\\AppData\\Local\\Temp\\tmp1vjjc3n7\\build\\tfx\\setup.log\n",
      "INFO:absl:Added --extra_package=C:\\Users\\deaston\\AppData\\Local\\Temp\\tmp1vjjc3n7\\build\\tfx\\dist\\tfx_ephemeral-0.30.0.tar.gz to beam args\n",
      "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
      "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
      "ERROR:absl:udf_utils.get_fn {'train_args': '{\\n  \"num_steps\": 100\\n}', 'eval_args': '{\\n  \"num_steps\": 5\\n}', 'custom_config': 'null', 'module_path': 'penguin_trainer@./pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl'} 'run_fn'\n",
      "INFO:absl:Installing './pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['C:\\\\Users\\\\deaston\\\\Anaconda3\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\deaston\\\\AppData\\\\Local\\\\Temp\\\\tmppkddhzm1', './pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed './pipelines\\\\penguin-tfma\\\\_wheels\\\\tfx_user_code_Trainer-0.0+07cdafd8f0f7a1ad6ac8588e2f5c805dcbb8053fdcd8989077ae568519d96699-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature body_mass_g has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature species has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Model: \"model_1\"\n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:culmen_length_mm (InputLayer)   [(None, 1)]          0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:culmen_depth_mm (InputLayer)    [(None, 1)]          0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:flipper_length_mm (InputLayer)  [(None, 1)]          0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:body_mass_g (InputLayer)        [(None, 1)]          0                                            \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:concatenate_1 (Concatenate)     (None, 4)            0           culmen_length_mm[0][0]           \n",
      "INFO:absl:                                                                 culmen_depth_mm[0][0]            \n",
      "INFO:absl:                                                                 flipper_length_mm[0][0]          \n",
      "INFO:absl:                                                                 body_mass_g[0][0]                \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_3 (Dense)                 (None, 8)            40          concatenate_1[0][0]              \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_4 (Dense)                 (None, 8)            72          dense_3[0][0]                    \n",
      "INFO:absl:__________________________________________________________________________________________________\n",
      "INFO:absl:dense_5 (Dense)                 (None, 3)            27          dense_4[0][0]                    \n",
      "INFO:absl:==================================================================================================\n",
      "INFO:absl:Total params: 139\n",
      "INFO:absl:Trainable params: 139\n",
      "INFO:absl:Non-trainable params: 0\n",
      "INFO:absl:__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 50s - loss: 1.0883 - sparse_categorical_accuracy: 0.55 - ETA: 0s - loss: 1.1364 - sparse_categorical_accuracy: 0.4491 - ETA: 0s - loss: 1.1154 - sparse_categorical_accuracy: 0.432 - ETA: 0s - loss: 1.0921 - sparse_categorical_accuracy: 0.432 - ETA: 0s - loss: 1.0643 - sparse_categorical_accuracy: 0.456 - ETA: 0s - loss: 1.0395 - sparse_categorical_accuracy: 0.479 - ETA: 0s - loss: 1.0093 - sparse_categorical_accuracy: 0.504 - ETA: 0s - loss: 0.9767 - sparse_categorical_accuracy: 0.529 - ETA: 0s - loss: 0.9428 - sparse_categorical_accuracy: 0.553 - ETA: 0s - loss: 0.9092 - sparse_categorical_accuracy: 0.574 - ETA: 0s - loss: 0.8797 - sparse_categorical_accuracy: 0.592 - 1s 8ms/step - loss: 0.8733 - sparse_categorical_accuracy: 0.5965 - val_loss: 0.0813 - val_sparse_categorical_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./pipelines\\penguin-tfma\\Trainer\\model\\23\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./pipelines\\penguin-tfma\\Trainer\\model\\23\\Format-Serving\\assets\n",
      "INFO:absl:Training complete. Model written to ./pipelines\\penguin-tfma\\Trainer\\model\\23\\Format-Serving. ModelRun written to ./pipelines\\penguin-tfma\\Trainer\\model_run\\23\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 23 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\23\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Model\"\n",
      ")], 'model_run': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model_run\\\\23\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")]}) for execution 23\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Trainer is finished.\n",
      "INFO:absl:Component Evaluator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "upstream_nodes: \"latest_blessed_model_resolver\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 24\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=24, input_dict={'examples': [Artifact(artifact: id: 25\n",
      "type_id: 6\n",
      "uri: \"./pipelines\\\\penguin-tfma\\\\CsvExampleGen\\\\examples\\\\21\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1624048269,sum_checksum:1624048269\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1624050294923\n",
      "last_update_time_since_epoch: 1624050294923\n",
      ", artifact_type: id: 6\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")], 'model': [Artifact(artifact: id: 26\n",
      "type_id: 9\n",
      "uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\23\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1624050309755\n",
      "last_update_time_since_epoch: 1624050309755\n",
      ", artifact_type: id: 9\n",
      "name: \"Model\"\n",
      ")], 'baseline_model': [Artifact(artifact: id: 8\n",
      "type_id: 9\n",
      "uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T13.17.45.885929:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1624047482245\n",
      "last_update_time_since_epoch: 1624047482245\n",
      ", artifact_type: id: 9\n",
      "name: \"Model\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Evaluator\\\\evaluation\\\\24\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Evaluator:evaluation:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Evaluator\\\\blessing\\\\24\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Evaluator:blessing:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}), exec_properties={'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}', 'example_splits': 'null'}, execution_output_uri='./pipelines\\\\penguin-tfma\\\\Evaluator\\\\.system\\\\executor_execution\\\\24\\\\executor_output.pb', stateful_working_dir='./pipelines\\\\penguin-tfma\\\\Evaluator\\\\.system\\\\stateful_working_dir\\\\18-06-2021T14.04.53.354211', tmp_dir='./pipelines\\\\penguin-tfma\\\\Evaluator\\\\.system\\\\executor_execution\\\\24\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "upstream_nodes: \"latest_blessed_model_resolver\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfma\"\n",
      ", pipeline_run_id='18-06-2021T14.04.53.354211')\n",
      "ERROR:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}', 'example_splits': 'null'} 'custom_eval_shared_model'\n",
      "INFO:absl:Adding default baseline ModelSpec based on the candidate ModelSpec provided. The candidate model will be called \"candidate\" and the baseline will be called \"baseline\": updated_config=\n",
      "model_specs {\n",
      "  name: \"candidate\"\n",
      "  label_key: \"species\"\n",
      "}\n",
      "model_specs {\n",
      "  name: \"baseline\"\n",
      "  label_key: \"species\"\n",
      "  is_baseline: true\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"species\"\n",
      "}\n",
      "metrics_specs {\n",
      "  per_slice_thresholds {\n",
      "    key: \"sparse_categorical_accuracy\"\n",
      "    value {\n",
      "      thresholds {\n",
      "        slicing_specs {\n",
      "        }\n",
      "        threshold {\n",
      "          value_threshold {\n",
      "            lower_bound {\n",
      "              value: 0.6\n",
      "            }\n",
      "          }\n",
      "          change_threshold {\n",
      "            absolute {\n",
      "              value: -1e-10\n",
      "            }\n",
      "            direction: HIGHER_IS_BETTER\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using ./pipelines\\penguin-tfma\\Trainer\\model\\23\\Format-Serving as candidate model.\n",
      "INFO:absl:Using ./pipelines\\penguin-tfma\\Trainer\\model\\8\\Format-Serving as baseline model.\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "ERROR:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}', 'example_splits': 'null'} 'custom_extractors'\n",
      "INFO:absl:Evaluation complete. Results written to ./pipelines\\penguin-tfma\\Evaluator\\evaluation\\24.\n",
      "INFO:absl:Checking validation results.\n",
      "INFO:absl:Blessing result True written to ./pipelines\\penguin-tfma\\Evaluator\\blessing\\24.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 24 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Evaluator\\\\evaluation\\\\24\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Evaluator:evaluation:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Evaluator\\\\blessing\\\\24\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Evaluator:blessing:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}) for execution 24\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Evaluator is finished.\n",
      "INFO:absl:Component Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\\\\\\penguin-tfma\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 25\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=25, input_dict={'model': [Artifact(artifact: id: 26\n",
      "type_id: 9\n",
      "uri: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\23\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1624050309755\n",
      "last_update_time_since_epoch: 1624050309755\n",
      ", artifact_type: id: 9\n",
      "name: \"Model\"\n",
      ")], 'model_blessing': [Artifact(artifact: id: 29\n",
      "type_id: 13\n",
      "uri: \"./pipelines\\\\penguin-tfma\\\\Evaluator\\\\blessing\\\\24\"\n",
      "custom_properties {\n",
      "  key: \"baseline_model\"\n",
      "  value {\n",
      "    string_value: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\8\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"baseline_model_id\"\n",
      "  value {\n",
      "    int_value: 8\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"./pipelines\\\\penguin-tfma\\\\Trainer\\\\model\\\\23\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 26\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Evaluator:blessing:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"0.30.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1624050333420\n",
      "last_update_time_since_epoch: 1624050333420\n",
      ", artifact_type: id: 13\n",
      "name: \"ModelBlessing\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"./pipelines\\\\penguin-tfma\\\\Pusher\\\\pushed_model\\\\25\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfma:18-06-2021T14.04.53.354211:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"PushedModel\"\n",
      ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"./serving_model\\\\\\\\penguin-tfma\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='./pipelines\\\\penguin-tfma\\\\Pusher\\\\.system\\\\executor_execution\\\\25\\\\executor_output.pb', stateful_working_dir='./pipelines\\\\penguin-tfma\\\\Pusher\\\\.system\\\\stateful_working_dir\\\\18-06-2021T14.04.53.354211', tmp_dir='./pipelines\\\\penguin-tfma\\\\Pusher\\\\.system\\\\executor_execution\\\\25\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"18-06-2021T14.04.53.354211\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfma.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"18-06-2021T14.04.53.354211\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfma.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\\\\\\penguin-tfma\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfma\"\n",
      ", pipeline_run_id='18-06-2021T14.04.53.354211')\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', 'C:\\\\Users\\\\deaston\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-1e777c5a-0bd6-4d17-b1b3-980ff015004f.json']\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir C:\\Users\\deaston\\Anaconda3\\lib\\site-packages\\tfx to temp dir C:\\Users\\deaston\\AppData\\Local\\Temp\\tmpwcgxhpvw\\build\\tfx\n",
      "INFO:absl:Cleaning up stateless execution info.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-473dfe731b93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m CustomLocalDagRunner().run(\n\u001b[0m\u001b[0;32m      2\u001b[0m   _create_pipeline(\n\u001b[0;32m      3\u001b[0m       \u001b[0mpipeline_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPELINE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mpipeline_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPELINE_ROOT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mdata_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-e879cc37abac>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, pipeline)\u001b[0m\n\u001b[0;32m     88\u001b[0m             custom_driver_spec=custom_driver_spec)\n\u001b[0;32m     89\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Component %s is running.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mcomponent_launcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Component %s is finished.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_execution_needed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[0mexecutor_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_executor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         execution_output = (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\launcher.py\u001b[0m in \u001b[0;36m_run_executor\u001b[1;34m(self, execution_info)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[0moutputs_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_output_dirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m       \u001b[0mexecutor_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_executor_operator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_executor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m       \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutor_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\python_executor_operator.py\u001b[0m in \u001b[0;36mrun_executor\u001b[1;34m(self, execution_info)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mexecutor_output_uri\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_output_uri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         stateful_working_dir=execution_info.stateful_working_dir)\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mexecutor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_executor_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrun_with_executor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tfx\\dsl\\components\\base\\base_executor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_beam_pipeline_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbeam\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         self._beam_pipeline_args = dependency_utils.make_beam_dependency_flags(\n\u001b[0m\u001b[0;32m    115\u001b[0m             self._beam_pipeline_args)\n\u001b[0;32m    116\u001b[0m         executor_class_path = '%s.%s' % (self.__class__.__module__,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tfx\\utils\\dependency_utils.py\u001b[0m in \u001b[0;36mmake_beam_dependency_flags\u001b[1;34m(beam_pipeline_args)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[0mabsl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempting to infer TFX Python dependency for beam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m   \u001b[0mdependency_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m   \u001b[0msdist_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_ephemeral_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m   \u001b[0mabsl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Added --extra_package=%s to beam args'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msdist_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m   \u001b[0mdependency_flags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--extra_package=%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msdist_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tfx\\utils\\dependency_utils.py\u001b[0m in \u001b[0;36mbuild_ephemeral_package\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m   absl.logging.info('Copying all content from install dir %s to temp dir %s',\n\u001b[0;32m    112\u001b[0m                     tfx_root_dir, tmp_dir)\n\u001b[1;32m--> 113\u001b[1;33m   \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfx_root_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tfx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m   \u001b[1;31m# Source directory default permission is 0555 but we need to be able to create\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m   \u001b[1;31m# new setup.py file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[0;32m    555\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    490\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[0;32m    493\u001b[0m                          dirs_exist_ok=dirs_exist_ok)\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[0;32m    555\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    490\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[0;32m    493\u001b[0m                          dirs_exist_ok=dirs_exist_ok)\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[0;32m    555\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    490\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[0;32m    493\u001b[0m                          dirs_exist_ok=dirs_exist_ok)\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[0;32m    555\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    490\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[0;32m    493\u001b[0m                          dirs_exist_ok=dirs_exist_ok)\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[0;32m    555\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    490\u001b[0m                         \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0msrcentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m                 copytree(srcobj, dstname, symlinks, ignore, copy_function,\n\u001b[0m\u001b[0;32m    493\u001b[0m                          dirs_exist_ok=dirs_exist_ok)\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[0;32m    555\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[1;31m# Will raise a SpecialFileError for unsupported file types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;31m# catch the Error from the recursive copytree so that we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# continue with other files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m             \u001b[1;31m# macOS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CustomLocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.proto import metadata_store_pb2\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "# TODO(b/171447278): Move these functions into the TFX library.\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
    "  context = metadata.store.get_context_by_type_and_name(\n",
    "      'node', f'{pipeline_name}.{component_id}')\n",
    "  executions = metadata.store.get_executions_by_context(context.id)\n",
    "  latest_execution = max(executions,\n",
    "                         key=lambda e:e.last_update_time_since_epoch)\n",
    "  return execution_lib.get_artifacts_dict(metadata, latest_execution.id, \n",
    "                                          metadata_store_pb2.Event.OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    }
   ],
   "source": [
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.metadata import Metadata\n",
    "from tfx.types import standard_component_specs\n",
    "\n",
    "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
    "    METADATA_PATH)\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "  # Find output artifacts from MLMD.\n",
    "  evaluator_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n",
    "                                          'Evaluator')\n",
    "  eval_artifact = evaluator_output[standard_component_specs.EVALUATION_KEY][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7a99752dda4a2ea5c217438eba28e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'species:0', 'metrics"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "eval_result = tfma.load_eval_result(eval_artifact.uri)\n",
    "tfma.view.render_slicing_metrics(eval_result, slicing_column='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
